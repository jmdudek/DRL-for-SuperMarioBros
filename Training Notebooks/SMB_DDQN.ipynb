{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HcBNsGd3ufJ"
      },
      "source": [
        "# **Imports and Installs**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For usage in Google Colab:"
      ],
      "metadata": {
        "id": "u6F1Na9oGWAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imageio-ffmpeg\n",
        "!pip install gym-super-mario-bros\n",
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "uaCdQOoBGVI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "va1Rp5_wGcbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports:"
      ],
      "metadata": {
        "id": "WBYrEQ1-Gd_0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-5SF-11XKMrc"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot') \n",
        "\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY # 5 actions\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT # 7 actions\n",
        "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT # 12 actions\n",
        "\n",
        "# For importing custom modules\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DRL_Group44/Modules')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment Creation and Preprocessing**\n",
        "\n"
      ],
      "metadata": {
        "id": "L9Eev_mjPcXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If run on Google Colab the imports will produce a reportMissingImports warning \n",
        "# However, everything works as intended and if run locally there are no warnings at all \n",
        "from helper_functions import make_env, preprocess_obs"
      ],
      "metadata": {
        "id": "xWGHYJJoatxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuzXxfB99YTl"
      },
      "source": [
        "# **Prioritized Experience Replay Buffer (PERB)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following PERB implementation is based on Schaul et al. (2016) https://arxiv.org/pdf/1511.05952.pdf, uses a sum tree and was adopted from this tutorial: https://adventuresinmachinelearning.com/prioritised-experience-replay/"
      ],
      "metadata": {
        "id": "p3EPL4lKWfuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from perb import PERB"
      ],
      "metadata": {
        "id": "2vr_z18Ia4TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWyblwWzOGqt"
      },
      "source": [
        "# **Policy, DDQN and Training Classes**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DDQN implementation according to van Hasselt et al. 2016: https://doi.org/10.1609/aaai.v30i1.10295"
      ],
      "metadata": {
        "id": "P17kOtvBchtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ddqn import Policy, DDQN, train_DDQN"
      ],
      "metadata": {
        "id": "rvio9ctNaHid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training**"
      ],
      "metadata": {
        "id": "oTtecENWC94p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The warnings while running the training can be ignored as we wanted to train on the original, unaltered SMB world (the other versions are simplified representations of it, see: https://github.com/Kautenja/gym-super-mario-bros#environments)."
      ],
      "metadata": {
        "id": "WcYE3gbNzCtR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_CNJ0rQ7LI0",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Set hyperparameters\n",
        "resize_env = (84,84)\n",
        "num_epochs = 2000001\n",
        "env_steps_per_epoch = 3\n",
        "batch_size = 64\n",
        "discount_factor = 0.9\n",
        "epsilon_decay_factor = 0.999\n",
        "min_epsilon = 0.1\n",
        "tau = 1/3750\n",
        "path = \"drive/MyDrive/DRL_Group44/SoftQ/lol\"\n",
        "video_steps = 1000\n",
        "saving_epoch = 5000\n",
        "plotting_epoch = 200\n",
        "transfer = False\n",
        "\n",
        "\n",
        "# Create environment\n",
        "num_skip = 4\n",
        "num_stack = 4\n",
        "reward_scale_factor = 600\n",
        "env = make_env(level=\"SuperMarioBros-1-1-v0\", # select -<world>-<stage>-v<version>\n",
        "               movement_type=RIGHT_ONLY, \n",
        "               num_skip=num_skip, \n",
        "               num_stack=num_stack, \n",
        "               reward_scale_factor=reward_scale_factor) \n",
        "img_dim = tf.shape(preprocess_obs(obs=env.reset(), resize_env=resize_env))\n",
        "\n",
        "# Initilaize the optimizer\n",
        "learning_rate = 0.00025\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Initialize policy and buffer\n",
        "epsilon = 0.3\n",
        "policy = Policy(eps=epsilon)\n",
        "buffer_size = 32000\n",
        "buffer = PERB(size=buffer_size, img_dim=img_dim)\n",
        "\n",
        "\n",
        "# Initialize model network and build it dynamically (according to enviroment's observation space)\n",
        "model_network = DDQN(env)\n",
        "model_network.build(input_shape=(None, img_dim[0], img_dim[1], img_dim[2]))\n",
        "\n",
        "# Initialize target network and build it dynamically (according to enviroment's observation space)\n",
        "target_network = DDQN(env)\n",
        "target_network.build(input_shape=(None, img_dim[0], img_dim[1], img_dim[2]))\n",
        "\n",
        "# Copy weights from model to target network\n",
        "target_network.set_weights(model_network.get_weights())\n",
        "\n",
        "\n",
        "# Train the model network and save the final weights\n",
        "train_DDQN(env=env,\n",
        "           model_network=model_network, \n",
        "           target_network=target_network, \n",
        "           policy=policy, \n",
        "           buffer=buffer, \n",
        "           optimizer=optimizer, \n",
        "           resize_env=resize_env,\n",
        "           num_epochs=num_epochs, \n",
        "           env_steps_per_epoch=env_steps_per_epoch, \n",
        "           batch_size=batch_size,  \n",
        "           discount_factor=discount_factor, \n",
        "           epsilon_decay_factor=epsilon_decay_factor, \n",
        "           min_epsilon=min_epsilon,\n",
        "           tau=tau, \n",
        "           path=path, \n",
        "           video_steps=video_steps, \n",
        "           saving_epoch=saving_epoch,\n",
        "           plotting_epoch=plotting_epoch,\n",
        "           transfer=transfer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9MkoZCKF2Nj"
      },
      "source": [
        "# **Transfer Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The warnings while running the training can be ignored as we wanted to train on the original, unaltered SMB world (the other versions are simplified representations of it, see: https://github.com/Kautenja/gym-super-mario-bros#environments)."
      ],
      "metadata": {
        "id": "o8BSxaC1zItB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnUtMeQ_F2Nj"
      },
      "outputs": [],
      "source": [
        "# Set hyperparameters\n",
        "resize_env = (84,84)\n",
        "num_epochs = 2000001\n",
        "env_steps_per_epoch = 3\n",
        "batch_size = 64\n",
        "discount_factor = 0.9\n",
        "epsilon_decay_factor = 0.999 \n",
        "min_epsilon = 0.1\n",
        "tau = 1/3750\n",
        "path = \"DDQN_PERB_Stack_Transfer\"\n",
        "video_steps = 1000\n",
        "saving_epoch = 5000\n",
        "plotting_epoch = 200\n",
        "transfer = True\n",
        "\n",
        "\n",
        "# Create environment\n",
        "num_skip = 4\n",
        "num_stack = 4\n",
        "reward_scale_factor = 600\n",
        "env = make_env(level=\"SuperMarioBros-1-2-v0\", # select -<world>-<stage>-v<version>\n",
        "               movement_type=RIGHT_ONLY, \n",
        "               num_skip=num_skip, \n",
        "               num_stack=num_stack, \n",
        "               reward_scale_factor=reward_scale_factor) \n",
        "img_dim = tf.shape(preprocess_obs(obs=env.reset(), resize_env=resize_env))\n",
        "\n",
        "# Initilaize the optimizer\n",
        "learning_rate = 0.00025\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Initialize policy and buffer\n",
        "epsilon = 0.3\n",
        "policy = Policy(eps=epsilon)\n",
        "buffer_size = 32000\n",
        "buffer = PERB(size=buffer_size, img_dim=img_dim)\n",
        "\n",
        "\n",
        "# Initialize model network and build it dynamically (according to enviroment's observation space), then load pretrained weights\n",
        "model_network = DDQN(env)\n",
        "model_network.build(input_shape=(None, img_dim[0], img_dim[1], img_dim[2]))\n",
        "model_network.load_weights(\"DDQN_PERB_Stack/SuperMarioBrosDDQNWeights2\")\n",
        "\n",
        "# Initialize target network and build it dynamically (according to enviroment's observation space)\n",
        "target_network = DDQN(env)\n",
        "target_network.build(input_shape=(None, img_dim[0], img_dim[1], img_dim[2]))\n",
        "\n",
        "# Copy weights from model to target network\n",
        "target_network.set_weights(model_network.get_weights())\n",
        "\n",
        "\n",
        "# Make only the last xx layers trainable\n",
        "for layer in model_network.layers[:-5]:\n",
        "    layer.trainable=False\n",
        "model_network.compile()\n",
        "\n",
        "for layer in target_network.layers[:-5]:\n",
        "    layer.trainable=False\n",
        "target_network.compile()\n",
        "\n",
        "\n",
        "# Train the model network and save the final weights\n",
        "train_DDQN(env=env, \n",
        "           model_network=model_network, \n",
        "           target_network=target_network, \n",
        "           policy=policy, \n",
        "           buffer=buffer, \n",
        "           optimizer=optimizer,\n",
        "           resize_env=resize_env, \n",
        "           num_epochs=num_epochs,\n",
        "           env_steps_per_epoch=env_steps_per_epoch, \n",
        "           batch_size=batch_size, \n",
        "           discount_factor=discount_factor,  \n",
        "           epsilon_decay_factor=epsilon_decay_factor, \n",
        "           min_epsilon=min_epsilon,\n",
        "           tau=tau, \n",
        "           path=path,\n",
        "           video_steps=video_steps, \n",
        "           saving_epoch=saving_epoch,\n",
        "           plotting_epoch=plotting_epoch,\n",
        "           transfer=transfer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**\n",
        "\n"
      ],
      "metadata": {
        "id": "rmtKyKOwHt4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluation import evaluation"
      ],
      "metadata": {
        "id": "b5aSf21UHw3F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"PathToData\"\n",
        "data_paths = [f\"{path}losses_rews_wins_SOFTQ_1-2_scratch.csv\", \n",
        "              f\"{path}losses_rews_wins_SOFTQ_1-2_all_earlier_50.csv\", \n",
        "              f\"{path}losses_rews_wins_SOFTQ_1-2_all_earlier_30.csv\", \n",
        "              f\"{path}losses_rews_wins_SOFTQ_1-2_all_earlier_10.csv\"]\n",
        "labels = [\"Scratch\", \"All_50\", \"All_30\", \"All_10\"]\n",
        "save = True"
      ],
      "metadata": {
        "id": "Tjvr7214jV6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode = \"reward\"\n",
        "avg_window = 500\n",
        "save_path = \"PathToSave\"\n",
        "evaluation(data_paths=data_paths, labels=labels, mode=mode, avg_window=avg_window, save=save, save_path=save_path)"
      ],
      "metadata": {
        "id": "L4I6l7kEI-4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode = \"loss\"\n",
        "avg_window = 15000\n",
        "save_path = \"PathToData\"\n",
        "evaluation(data_paths=data_paths, labels=labels, avg_window=avg_window, mode=mode, save=save, save_path=save_path)"
      ],
      "metadata": {
        "id": "9mwrYr0cPuWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode = \"win\"\n",
        "avg_window=500\n",
        "save_path = \"PathToData\"\n",
        "evaluation(data_paths=data_paths, labels=labels, mode=mode, avg_window=avg_window, save=save, save_path=save_path)"
      ],
      "metadata": {
        "id": "A4nXSyqHPyXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}