{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HcBNsGd3ufJ"
      },
      "source": [
        "# **Imports and Installs**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For usage in Google Colab:"
      ],
      "metadata": {
        "id": "wHf6ihMKFkng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imageio-ffmpeg\n",
        "!pip install gym-super-mario-bros\n",
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "ALZXtd7RSLhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "MiHcWKf8ePlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports:"
      ],
      "metadata": {
        "id": "Zu4WlDMBGGet"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5SF-11XKMrc"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot') \n",
        "\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY # 5 actions\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT # 7 actions\n",
        "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT # 12 actions\n",
        "\n",
        "# For importing custom modules\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DRL_Group44/Modules')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment Creation and Preprocessing**"
      ],
      "metadata": {
        "id": "ASZVM9SPPeZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If run on Google Colab the imports will produce a reportMissingImports warning \n",
        "# However, everything works as intended and if run locally there are no warnings at all\n",
        "from helper_functions import make_env, preprocess_obs"
      ],
      "metadata": {
        "id": "IP01jwHlaE2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuzXxfB99YTl"
      },
      "source": [
        "# **Prioritized Experience Replay Buffer (PERB)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following PERB implementation is based on Schaul et al. (2016) https://arxiv.org/pdf/1511.05952.pdf, uses a sum tree and was adopted from this tutorial: https://adventuresinmachinelearning.com/prioritised-experience-replay/"
      ],
      "metadata": {
        "id": "XjXFaXC5WfEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from perb import PERB"
      ],
      "metadata": {
        "id": "gpH5MQBoaDSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWyblwWzOGqt"
      },
      "source": [
        "# **Policy, SoftQN and Training Classes**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SoftQN implementation according to Haarnoja et al. (2017) https://arxiv.org/abs/1702.08165"
      ],
      "metadata": {
        "id": "XBG6tjqrcLoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from softqn import Policy, SoftQN, train_SoftQN"
      ],
      "metadata": {
        "id": "YpObde-UafR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training**\n"
      ],
      "metadata": {
        "id": "uZE_ttm5C8N6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The warnings while running the training can be ignored as we wanted to train on the original, unaltered SMB world (the other versions are simplified representations of it, see: https://github.com/Kautenja/gym-super-mario-bros#environments)."
      ],
      "metadata": {
        "id": "gkqg-vohzDct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_CNJ0rQ7LI0",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Set hyperparameters\n",
        "resize_env = (84,84)\n",
        "num_epochs = 2000001\n",
        "env_steps_per_epoch = 3\n",
        "batch_size = 64\n",
        "learning_rate = 0.00025\n",
        "discount_factor = 0.9\n",
        "entropy_factor = 1 / 600\n",
        "tau = 1/3750\n",
        "#path = \"drive/MyDrive/DRL_Group44/SoftQ/Transfer_Learning/Pre_Trained_Differential\"\n",
        "path = \"drive/MyDrive/DRL_Group44/SoftQ/lol\"\n",
        "video_steps=1000\n",
        "saving_epoch=5000\n",
        "plotting_epoch=200\n",
        "transfer = False\n",
        "\n",
        "\n",
        "# Initialize the optimizer\n",
        "learning_rate = 0.00025\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Create environment\n",
        "num_skip = 4\n",
        "num_stack = 4\n",
        "reward_scale_factor = 600\n",
        "env = make_env(level=\"SuperMarioBros-1-1-v0\", # Select -<world>-<stage>-v<version>\n",
        "               movement_type=RIGHT_ONLY, \n",
        "               num_skip=num_skip, \n",
        "               num_stack=num_stack, \n",
        "               reward_scale_factor=reward_scale_factor) \n",
        "img_dim = tf.shape(preprocess_obs(obs=env.reset(), resize_env=resize_env))\n",
        "\n",
        "# Initialize Policy and buffer\n",
        "policy = Policy(heat_param=1/600) # heat_param noch zu tunen\n",
        "buffer_size = 64\n",
        "buffer = PERB(size=buffer_size, img_dim=img_dim)\n",
        "\n",
        "\n",
        "# Initialize model network and build it dynamically (according to enviroment's observation space)\n",
        "model_network = SoftQN(env)\n",
        "model_network.build(input_shape=(None, img_dim[0], img_dim[1], img_dim[2]))\n",
        "\n",
        "# Initialize target network and build it dynamically (according to enviroment's observation space)\n",
        "target_network = SoftQN(env)\n",
        "target_network.build(input_shape=(None, img_dim[0], img_dim[1], img_dim[2]))\n",
        "\n",
        "# Copy weights from model to target network\n",
        "target_network.set_weights(model_network.get_weights())\n",
        "\n",
        "\n",
        "# Train the model network and save the final weights\n",
        "train_SoftQN(env=env, \n",
        "             model_network=model_network, \n",
        "             target_network=target_network, \n",
        "             policy=policy, \n",
        "             buffer=buffer, \n",
        "             optimizer=optimizer, \n",
        "             resize_env=resize_env,\n",
        "             num_epochs=num_epochs, \n",
        "             env_steps_per_epoch=env_steps_per_epoch,\n",
        "             batch_size=batch_size, \n",
        "             discount_factor=discount_factor, \n",
        "             tau=tau, \n",
        "             path=path,\n",
        "             video_steps=video_steps,\n",
        "             saving_epoch=saving_epoch,\n",
        "             plotting_epoch=plotting_epoch,\n",
        "             entropy_factor=entropy_factor,\n",
        "             transfer=transfer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer Learning**"
      ],
      "metadata": {
        "id": "1Q2jVStOWF_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The warnings while running the training can be ignored as we wanted to train on the original, unaltered SMB world (the other versions are simplified representations of it, see: https://github.com/Kautenja/gym-super-mario-bros#environments)."
      ],
      "metadata": {
        "id": "hRk3D_oAzInC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "resize_env = (84,84)\n",
        "num_epochs = 2000001\n",
        "env_steps_per_epoch = 3\n",
        "batch_size = 64\n",
        "discount_factor = 0.9\n",
        "entropy_factor = 1 / 600\n",
        "tau = 1/3750\n",
        "path = \"drive/MyDrive/DRL_Group44/SoftQ/Transfer_Learning/Pre_Trained_00025_all_earlier_10\"\n",
        "video_steps=1000\n",
        "saving_epoch=5000\n",
        "plotting_epoch=200\n",
        "transfer = True\n",
        "\n",
        "\n",
        "# Initialize the optimizer\n",
        "learning_rate = 0.00025\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Create environment\n",
        "num_skip = 4\n",
        "num_stack = 4\n",
        "reward_scale_factor = 600\n",
        "env = make_env(level=\"SuperMarioBros-1-2-v0\",  # Select -<world>-<stage>-v<version>\n",
        "               movement_type=RIGHT_ONLY, \n",
        "               num_skip=num_skip, \n",
        "               num_stack=num_stack, \n",
        "               reward_scale_factor=reward_scale_factor)\n",
        "img_dim = tf.shape(preprocess_obs(obs=env.reset(), resize_env=resize_env))\n",
        "\n",
        "# Initialize Policy and buffer\n",
        "policy = Policy(heat_param=1/600)\n",
        "buffer_size=32000\n",
        "buffer = PERB(size=buffer_size, img_dim=img_dim)\n",
        "\n",
        "\n",
        "# Initialize model network and build it dynamically (according to enviroment's observation space), then load pretrained weights\n",
        "model_network = SoftQN(env)\n",
        "model_network.build(input_shape=(None, img_dim[0], img_dim[1], img_dim[2]))\n",
        "model_network.load_weights(\"drive/MyDrive/DRL_Group44/SoftQ/Transfer_Learning/Pre_Trained_00025_all_earlier_10/SuperMarioBrosDQNWeights2_epoch_185000\")\n",
        "\n",
        "# Initialize target network and build it dynamically (according to enviroment's observation space)\n",
        "target_network = SoftQN(env)\n",
        "target_network.build(input_shape=(None, img_dim[0], img_dim[1], img_dim[2]))\n",
        "\n",
        "# Copy weights from model to target network\n",
        "target_network.set_weights(model_network.get_weights())\n",
        "\n",
        "\n",
        "# # Make only the last xx layers trainable\n",
        "# for layer in model_network.layers[:-5]:\n",
        "#     layer.trainable=False\n",
        "# model_network.compile()\n",
        "\n",
        "# for layer in target_network.layers[:-5]:\n",
        "#     layer.trainable=False\n",
        "# target_network.compile()\n",
        "\n",
        "\n",
        "# Train the model network and save the final weights\n",
        "train_SoftQN(env=env, \n",
        "             model_network=model_network, \n",
        "             target_network=target_network, \n",
        "             policy=policy, \n",
        "             buffer=buffer, \n",
        "             optimizer=optimizer, \n",
        "             resize_env=resize_env,\n",
        "             num_epochs=num_epochs, \n",
        "             env_steps_per_epoch=env_steps_per_epoch,\n",
        "             batch_size=batch_size, \n",
        "             discount_factor=discount_factor, \n",
        "             tau=tau, \n",
        "             path=path,\n",
        "             video_steps=video_steps,\n",
        "             saving_epoch=saving_epoch,\n",
        "             plotting_epoch=plotting_epoch,\n",
        "             entropy_factor=entropy_factor,\n",
        "             transfer=transfer)"
      ],
      "metadata": {
        "id": "h6rq7thzV7Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**\n",
        "\n"
      ],
      "metadata": {
        "id": "rmtKyKOwHt4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluation import evaluation"
      ],
      "metadata": {
        "id": "b5aSf21UHw3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"PathToData\"\n",
        "data_paths = [f\"{path}losses_rews_wins_SOFTQ_1-2_scratch.csv\", \n",
        "              f\"{path}losses_rews_wins_SOFTQ_1-2_all_earlier_50.csv\", \n",
        "              f\"{path}losses_rews_wins_SOFTQ_1-2_all_earlier_30.csv\", \n",
        "              f\"{path}losses_rews_wins_SOFTQ_1-2_all_earlier_10.csv\"]\n",
        "labels = [\"Scratch\", \"All_50\", \"All_30\", \"All_10\"]\n",
        "save = True"
      ],
      "metadata": {
        "id": "X__PAAlSjT95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode = \"reward\"\n",
        "avg_window = 500\n",
        "save_path = \"PathToSave\"\n",
        "evaluation(data_paths=data_paths, labels=labels, mode=mode, avg_window=avg_window, save=save, save_path=save_path)"
      ],
      "metadata": {
        "id": "L4I6l7kEI-4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode = \"loss\"\n",
        "avg_window = 15000\n",
        "save_path = \"PathToSave\"\n",
        "evaluation(data_paths=data_paths, labels=labels, avg_window=avg_window, mode=mode, save=save, save_path=save_path)"
      ],
      "metadata": {
        "id": "9mwrYr0cPuWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode = \"win\"\n",
        "avg_window=500\n",
        "save_path = \"PathToSave\"\n",
        "evaluation(data_paths=data_paths, labels=labels, mode=mode, avg_window=avg_window, save=save, save_path=save_path)"
      ],
      "metadata": {
        "id": "A4nXSyqHPyXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}